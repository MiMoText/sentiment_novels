{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import os\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm\n",
    "from scipy.stats import ks_2samp\n",
    "from IPython.core.display import display, HTML\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = [20, 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The execution of the following cell is mandatory! This is needed to have progress bars with pandas computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\anaconda\\lib\\site-packages\\tqdm\\std.py:668: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n"
     ]
    }
   ],
   "source": [
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have not yet downloaded the Treebank tokenizer for nltk, please do so by executing the following cell. You only need to do this once!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "*language:* The language of your texts and the dictionaries you want to use.\n",
    "\n",
    "*dictionary_dir:* The path to the directory containing the dictionaries. If you use the default directory structure, you can use `dictionaries/manual/` for dictionaries containing only manual annotated words and `dictionaries/computational/` for dictionaries containing manual and automatically extended words. Please make sure to use a '/' (slash) in the end. For example: `path/to/dictionaries/`.\n",
    "\n",
    "*dataframe_filename:* The full path to the filename of the pandas DataFrame created in the previous step. You may use the .p extension indicating a pickled file.\n",
    "\n",
    "*texts_results_dir:* The path to the directory used for text outputs. Please make sure to use a '/' (slash) in the end. For example: `path/to/output/`.\n",
    "\n",
    "*plot_results_dir:* The path to the directory used for plot outputs. Please make sure to use a '/' (slash) in the end. For example: `path/to/output/`.\n",
    "\n",
    "*plot_file_format:* The file format to use for your plots. Typically, matplotlib supports: `png`, `pdf`, `ps`, `eps` and `svg`.\n",
    "\n",
    "*output_dataframe_filename:* The full path to the filename of the resulting pandas DataFrame including the computed sentiment. You may use the **.p** extension indicating a pickled file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "language = \"french\"\n",
    "dictionary_dir = \"dictionaries/computational_corrected/\"\n",
    "dataframe_filename = \"texts_mimotext.p\"\n",
    "text_results_dir = \"results/texts/mimotext\"\n",
    "plot_results_dir = \"results/plots/mimotext/\"\n",
    "plot_file_format= \"pdf\"\n",
    "output_dataframe_filename = \"texts_with_sentiment.p\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Directory Setup (Optional)\n",
    "Creates directories according to the configuration if not already created manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(dictionary_dir):\n",
    "    os.makedirs(dictionary_dir)\n",
    "if not os.path.exists(text_results_dir):\n",
    "    os.makedirs(text_results_dir)\n",
    "if not os.path.exists(plot_results_dir):\n",
    "    os.makedirs(plot_results_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 2272 negative words\n",
      "loaded 4216 positive words\n"
     ]
    }
   ],
   "source": [
    "sentiment_dict = {}\n",
    "with open(\"{}{}_negative.txt\".format(dictionary_dir, language.lower()), \"r\") as fr:\n",
    "    sentiment_dict[\"neg\"] = fr.read().splitlines()\n",
    "with open(\"{}{}_positive.txt\".format(dictionary_dir, language.lower()), \"r\") as fr:\n",
    "    sentiment_dict[\"pos\"] = fr.read().splitlines()\n",
    "\n",
    "print(\"loaded {} negative words\".format(len(sentiment_dict[\"neg\"])))\n",
    "print(\"loaded {} positive words\".format(len(sentiment_dict[\"pos\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded dataframe with 186 texts and 2 attributes\n"
     ]
    }
   ],
   "source": [
    "texts_df = pd.read_pickle(dataframe_filename)\n",
    "print(\"loaded dataframe with {} texts and {} attributes\".format(texts_df.shape[0], texts_df.shape[1] - 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute sentiment\n",
    "This functions computes the sentiment of texts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_sentiment(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [t.lower() for t in tokens]\n",
    "    num_negative = 0\n",
    "    num_positive = 0\n",
    "    for nw in sentiment_dict[\"neg\"]:\n",
    "        num_negative += tokens.count(nw.lower())\n",
    "    for pw in sentiment_dict[\"pos\"]:\n",
    "        num_positive += tokens.count(pw.lower())\n",
    "    try:\n",
    "        sentiment_score = (num_positive - num_negative) / (num_positive + num_negative)\n",
    "    except ZeroDivisionError:\n",
    "        sentiment_score = 0\n",
    "    return sentiment_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply `compute_sentiment` to each text cell in the DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66cf5794d7024455b5a0ec68dc99b08c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=186.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "texts_df[\"sentiment\"] = texts_df[\"text\"].progress_apply(compute_sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the DataFrame including the computing sentiment for later use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_df.to_pickle(output_dataframe_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overall Sentiment Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This prints descriptive statistics of sentiment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "texts_df[\"sentiment\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Histogram Plot\n",
    "Note that all plots are also individually saved to `plot_results_dir`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "texts_df[\"sentiment\"].plot(kind=\"hist\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"{}sentiment_histogram.{}\".format(plot_results_dir, plot_file_format))\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attribute Plots\n",
    "Set the attribute you want to analyze:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attribute = \"year\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_df.groupby(attribute)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bar Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ax = texts_df.groupby(attribute)[\"sentiment\"].mean().plot(kind=\"bar\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"{}{}_bar_plot.{}\".format(plot_results_dir, attribute, plot_file_format))\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Box Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_df.boxplot(\"sentiment\", by=attribute)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"{}{}_box_plot.{}\".format(plot_results_dir, attribute, plot_file_format))\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Line Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_df.groupby(attribute)[\"sentiment\"].mean().plot(kind=\"line\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"{}{}_line_plot.{}\".format(plot_results_dir, attribute, plot_file_format))\n",
    "plt.grid()\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Seaborn Lineplot\n",
    "Seaborn `lineplot` ([documentation](https://seaborn.pydata.org/generated/seaborn.lineplot.html)) allows to separate attributes very easily using the `hue` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.lineplot(data=texts_df, x=attribute, y=\"sentiment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Individual Text Analysis\n",
    "Set the filename of the individual text you want to analyze:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"Bellin_Nuit.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentiment Development"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell allows you to automalitcally partition text into cunks and compute sentiment seperately for each chunk.\n",
    "Please set `num_chunks` to the number of chunks you want to analyze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_chunks = 30\n",
    "tokens = nltk.word_tokenize(texts_df.loc[filename, \"text\"])\n",
    "\n",
    "chunks = []\n",
    "\n",
    "for i in range(0, num_chunks):\n",
    "    chunks.append(tokens[i::num_chunks])\n",
    "\n",
    "distant_results = []\n",
    "for c in chunks:\n",
    "    num_positive = 0\n",
    "    num_negative = 0\n",
    "    for nw in sentiment_dict[\"neg\"]:\n",
    "        num_negative += c.count(nw.lower())\n",
    "    for pw in sentiment_dict[\"pos\"]:\n",
    "        num_positive += c.count(pw.lower())\n",
    "    try:\n",
    "        sentiment_score = (num_positive - num_negative) / (num_positive + num_negative)\n",
    "    except ZeroDivisionError:\n",
    "        sentiment_score = 0\n",
    "    distant_results.append({\"num_positive\": num_positive, \"num_negative\": num_negative, \"sentiment_score\": sentiment_score})\n",
    "\n",
    "distant_results_df = pd.DataFrame(distant_results)\n",
    "\n",
    "lst_pal = {\"num_positive\": \"g-\", \"num_negative\": \"r-\", \"sentiment_score\": \"k--\"}\n",
    "\n",
    "ax1 = distant_results_df[[\"num_positive\", \"num_negative\"]].plot(style=lst_pal, legend=False)\n",
    "ax1.set_xlabel(\"Text Progress ->\")\n",
    "ax1.set_ylabel(\"Number of Words\")\n",
    "\n",
    "ax2 = distant_results_df[\"sentiment_score\"].plot(secondary_y=True, style=lst_pal)\n",
    "ax2.set_ylabel(\"Sentiment Score\")\n",
    "\n",
    "ax1.grid()\n",
    "ax2.grid(linestyle=\"--\")\n",
    "\n",
    "handles,labels = [],[]\n",
    "for ax in [ax1, ax2]:\n",
    "    for h,l in zip(*ax.get_legend_handles_labels()):\n",
    "        handles.append(h)\n",
    "        labels.append(l)\n",
    "\n",
    "plt.legend(handles, labels)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"{}{}_distant.{}\".format(plot_results_dir, filename.replace(\".\", \"_\"), plot_file_format))\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentiment Words Highlighting\n",
    "The next cell highlights all the words conveying a sentiment in your individual text file. This is also saved to `texts_results_dir` as an *.html* file for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_to_print = texts_df.loc[filename, \"text\"]\n",
    "for nw in sentiment_dict[\"neg\"]:\n",
    "    if nw in text_to_print and nw not in [\"span\", \"style\", \"color\", \"font\", \"size\"]:\n",
    "        text_to_print = re.sub(r\"\\b{}\\b\".format(nw), r\"<span style='color:#E74C3C; font-size:20pt'><b>{}</b></span>\".format(nw), text_to_print)\n",
    "        \n",
    "for pw in sentiment_dict[\"pos\"]:\n",
    "    if pw in text_to_print and pw not in [\"span\", \"style\", \"color\", \"font\", \"size\"]:\n",
    "        text_to_print = re.sub(r\"\\b{}\\b\".format(pw), r\"<span style='color:#27AE60; font-size:20pt'><b>{}</b></span>\".format(pw), text_to_print)\n",
    "\n",
    "with open(\"{}{}.html\".format(text_results_dir, \".\".join(filename.split(\".\")[:-1])), \"w\") as outfile:\n",
    "    outfile.write(text_to_print)\n",
    "HTML(text_to_print)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save.HTML(text_to_print)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothesis Test Example "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following two-sample Kolmogorov-Smirnov has the null hypothesis that two samples are drawn from the same distribution (i.e., the sentiment is indentical for both). If the resulting p-value is smaller than, for example, the commonly used significance level 0.05, we can reject the null hypothesis. This would suggest that there are significant differences in sentiment between the two distributions. We now compare the two italian periodicals using `ks_2samp` ([documentation](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.ks_2samp.html)) from scipy. Please make sure that you set `language = italian` and that you loaded the correct texts as well as sentiment dictionaries before executing this cell. First we separate the two disttributions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlieryear = texts_df[texts_df[\"year\"] == 1751][\"sentiment\"]\n",
    "lateryear = texts_df[texts_df[\"year\"] == 1764 ][\"sentiment\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlieryear.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lateryear.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then we conduct our hypothesis test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "statistic, pvalue = ks_2samp(abbes_voyage, voltaire_candide)\n",
    "print(\"pvalue:\", pvalue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since 0.28 > 0.05 (our significance level) we cannot reject our null hypothesis and, thus, the difference in sentiment between the two periodicals is non-significant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference\n",
    "\n",
    "Koncar, P., Druml, L., Ertler, K.-D., Fuchs, A., Geiger, B. C., Glatz, C., Hobisch, E., Mayer, P., Saric, S., Scholger, M. & Voelkl, Y. (2021) A Sentiment Tool Chain for Languages of the 18th Century. https://github.com/philkon/sentiment-tool-chai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
